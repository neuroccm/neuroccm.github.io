<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="https://www.w3.org/2005/Atom">
  <channel>
    <title>SiLab</title>
    <description>Stroke Innovation Lab is the lab website of Dr. Khosravani, highlighting our initiatives, projects, and blog site related to stroke and critical care/neurocritical care.
</description>
    <link>http://0.0.0.0:4000/</link>
    <atom:link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 26 Jan 2025 22:08:29 -0600</pubDate>
    <lastBuildDate>Sun, 26 Jan 2025 22:08:29 -0600</lastBuildDate>
    <generator>Jekyll v4.2.2</generator>
    
      <item>
        <title>See-2-Sound: How Spatial Audio has potential for clincial applications</title>
        <description>&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;See-2-Sound: How Spatial Audio has potential for clincial applications&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Work led by: Rishit Dagli, CS, University of Toronto&lt;/p&gt;

&lt;p&gt;The world of generative AI is constantly expanding, with models now capable of creating high-resolution content across multiple modalities, including images, text, speech, and video. However, one area that has lagged behind is the generation of high-quality spatial audio that complements these visuals. This is where SEE-2-SOUND comes in, a novel approach that generates spatial audio from images, animated images, and videos.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bridging the Gap Between Visuals and Immersive Audio&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;SEE-2-SOUND is designed to fill the gap in generating spatial audio, which is crucial for creating truly immersive experiences. Current audio generation models often excel in producing natural audio, speech, or music, but they often fall short in integrating the spatial cues needed for realistic sound perception. The ability to pinpoint the location of a sound source is a key element of human perception, and SEE-2-SOUND aims to replicate this in generated audio.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How Does SEE-2-SOUND Work?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The SEE-2-SOUND method works by breaking down the process into several key stages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Source Estimation:&lt;/strong&gt; The model first identifies regions of interest within the input visual content (image or video). It then estimates the 3D positions of these regions on a viewing sphere. This process includes using a monocular depth map to refine the spatial information.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mono Audio Generation:&lt;/strong&gt; For each identified region of interest, the model generates a mono audio clip using a pre-trained CoDi model. The audio can also be conditioned on a text prompt.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Spatial Audio Integration:&lt;/strong&gt; The generated mono audio clips are combined with the spatial information to create a 4D representation for each region. The model then places these sound sources in a virtual room and computes Room Impulse Responses (RIRs) for each source-microphone pair. The microphones are positioned according to the 5.1 channel configuration, ensuring compatibility with common audio systems. This generates a 5.1 surround sound spatial audio output.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Zero-Shot Approach&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A key advantage of SEE-2-SOUND is that it is a &lt;strong&gt;zero-shot approach&lt;/strong&gt;. This means that it can generate spatial audio without needing specific training data for every type of visual input. This makes it highly versatile and applicable to a wide range of content, including images from the web, videos generated by models like OpenAI’s Sora, and other dynamic visuals.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Evaluation and Results&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Evaluating spatial audio generation is challenging, as there are no direct metrics to measure its quality. Therefore, the researchers employed a combination of methods to assess their approach:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Human Evaluation:&lt;/strong&gt; Human evaluators rated the realism, immersion, and accuracy of generated audio when paired with visual content using semantic differential scales. They also performed tasks such as identifying the direction and distance of sounds and matching audio clips to their corresponding images or videos.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Marginal Scene Guidance:&lt;/strong&gt; A new evaluation protocol was developed to measure how well the generated audio is guided by the visual scene. This protocol uses another model, AViTAR, to modify audio to match the image, and then assesses the similarity between the modified audio and the original generated audio.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The results from these evaluations indicate that SEE-2-SOUND performs well in generating compelling spatial audio.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Future Directions and Potential Applications&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;While SEE-2-SOUND shows promising results, there are several avenues for future improvement:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Fine Details:&lt;/strong&gt; The model may not detect all fine details in images and videos and does not produce audio for every detail.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Motion Cues:&lt;/strong&gt; Currently, the model does not generate audio based on motion cues and adding motion backbones might improve the results.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Real-Time Capabilities:&lt;/strong&gt; The method does not currently work in real time on an Nvidia A100-80 GPU. However, using other models to solve the subproblems might bring it to real-time capabilities.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Despite these limitations, the potential applications of SEE-2-SOUND are vast:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Enhancing Generated Visuals:&lt;/strong&gt; It can add spatial audio to images and videos generated by AI models, making them more immersive.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Interactive Real Images:&lt;/strong&gt; It can make real images interactive through sound.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Human-Computer Interaction:&lt;/strong&gt; It can improve human-computer interaction by adding realistic spatial audio cues.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Accessibility:&lt;/strong&gt; It can enhance accessibility by providing audio information about visual content.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;A Step Towards Complete Generation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;SEE-2-SOUND is a step towards truly complete generation, bridging the gap between visual and auditory experiences. By enabling the creation of spatial audio from visual content, it opens up exciting new possibilities for immersive content creation and interaction. To the best of the authors’ knowledge, this approach is the first to generate spatial audio from images and videos. The team hopes to inspire future work that will lead to the generation of truly immersive digital content.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relevant Links&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Paper:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2406.06612&quot;&gt;arxiv.com&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Project Website:&lt;/strong&gt; &lt;a href=&quot;https://rishit-dagli.github.io/2024/06/18/s2s.html&quot;&gt;github.io&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 01 Aug 2024 09:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/ml,/tech,/stroke/2024/08/01/See-2-Sound.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/ml,/tech,/stroke/2024/08/01/See-2-Sound.html</guid>
        
        
        <category>ML,</category>
        
        <category>Tech,</category>
        
        <category>Stroke</category>
        
      </item>
    
      <item>
        <title>Tuning In: How Audio Analysis is Revolutionizing Clinical Diagnostics</title>
        <description>&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tuning In: How Audio Analysis is Revolutionizing Clinical Diagnostics&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Main contributors: Hamza Mahdi * , Eptehal Nashnoush * , Rishit Dagli (*Authors contributed equally)&lt;/p&gt;

&lt;p&gt;Post By: (NotebookLM, summarized) work by team MASA&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.10100&quot;&gt;Read the full arXiv paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the realm of healthcare, audio biomarkers are emerging as powerful tools for diagnosis and monitoring. From detecting respiratory problems to assessing neurological conditions, sound analysis is proving to be a versatile and non-invasive approach. Recent advancements in machine learning are enhancing the capabilities of audio classification, but how do these models perform in real-world clinical settings with limited data? A recent study, “Tuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data,” delves into this very question, providing valuable insights into the nuances of audio-based clinical diagnostics.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Challenge of Limited Data&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One of the major hurdles in applying machine learning to clinical data is the scarcity of large, high-quality datasets. This is especially true for rare diseases or when collecting data prospectively. This study addresses this challenge by analyzing the performance of various deep learning models on &lt;strong&gt;two novel, prospectively collected audio datasets from stroke patients&lt;/strong&gt;. These datasets include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Dataset NIHSS:&lt;/strong&gt; Captures continuous speech, sentences, and words based on the National Institutes of Health Stroke Scale (NIHSS), a standard neurological assessment tool.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dataset Vowel:&lt;/strong&gt; A unique dataset of sustained vowel sounds from patients, aiding in the analysis of swallowing disorders.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These datasets are first-of-their-kind, addressing a critical gap in research related to disease state classification with limited real-world data. Due to patient privacy regulations, this clinical data is not publicly available at this time, but the researchers are working to make it available in the near future.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model Selection and Preprocessing: Key Factors&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The study compares various models, including Convolutional Neural Networks (CNNs) like &lt;strong&gt;DenseNet&lt;/strong&gt; and &lt;strong&gt;ConvNeXt&lt;/strong&gt;, and transformer models such as &lt;strong&gt;ViT&lt;/strong&gt; and &lt;strong&gt;SWIN&lt;/strong&gt;. It also includes pre-trained audio models like &lt;strong&gt;AST, YAMNet&lt;/strong&gt;, and &lt;strong&gt;VGGish&lt;/strong&gt;. A key focus of the study is the impact of preprocessing techniques on model performance. The researchers explored three primary methods:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Mel RGB:&lt;/strong&gt; Spectrograms converted to RGB images using color maps.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mel mono:&lt;/strong&gt; Grayscale spectrograms.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Superlet:&lt;/strong&gt; A relatively recent method for transforming time-series data into spectrograms that preserves both time and frequency resolution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Key Findings&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The research revealed several interesting findings:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;CNNs Can Compete with Transformers:&lt;/strong&gt; In small dataset contexts, CNNs like DenseNet and ConvNeXt can match or even exceed the performance of transformer models. Specifically, DenseNet-Contrastive and AST models showed notable performance.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pretraining is Crucial:&lt;/strong&gt; Pretraining on large datasets such as ImageNet, AudioSet, US8K, and ESC50 is essential for enhancing the performance of models on smaller, specific clinical datasets.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Preprocessing Matters:&lt;/strong&gt; The study found that RGB and grayscale spectrogram transformations affect model performance differently depending on the priors they learn from pretraining.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The Role of Color:&lt;/strong&gt;  Surprisingly, RGB pre-processing, often used with ImageNet pretraining, outperformed grayscale triple-channel approaches, likely because the convolutional layers of models pretrained on ImageNet are more attuned to features in RGB images.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;AST Model Efficiency:&lt;/strong&gt; The AST model achieved optimal results with only 6 epochs of training, highlighting the potential for efficient training of transformer models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model Specific Performance&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;ConvNeXt (Mel RGB)&lt;/strong&gt;: Showcased robust performance across metrics (AUC of 0.91, sensitivity of 0.78, and specificity of 0.89).&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;DenseNet (Mel RGB)&lt;/strong&gt;: Achieved high sensitivity (0.89), but with a slightly lower precision compared to ConvNeXt.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;DenseNet Contrastive US8K (Mel mono)&lt;/strong&gt;:  Demonstrated exceptional performance with perfect specificity and precision, resulting in the highest F1 score of 0.88.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Implications for Clinical Practice&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This study underscores the significance of strategic model selection, pretraining, and preprocessing for audio-based diagnostics. The findings suggest that in clinical settings where data is often limited, a &lt;strong&gt;standardized, contextually tailored approach to preprocessing can significantly improve the performance of deep learning models&lt;/strong&gt;. Moreover, the robustness of transformer models in handling limited training epochs opens avenues for refining audio classification frameworks. By carefully choosing preprocessing techniques and models, healthcare providers can enhance diagnostic accuracy and efficiency in various clinical environments.&lt;/p&gt;

&lt;p&gt;This research has implications for a variety of conditions, including stroke, other neurological conditions, and rare diseases, where data scarcity is an intrinsic challenge. The use of audio as a biomarker for swallowing status, as demonstrated in this study, highlights the potential of this approach for broader applications in clinical settings.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Future Directions&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The researchers emphasize the need to consider various confounding factors, such as age, gender, stroke severity, and other medical conditions, in future studies. Stratified analysis can provide more detailed insights into the performance of preprocessing techniques across varied patient demographics, refining the clinical utility of audio classifiers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The study “Tuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data” provides valuable insights into the effective application of deep learning models for clinical audio classification. By focusing on the significance of pretraining, preprocessing, and model selection, this work paves the way for more accurate and efficient audio-based diagnostic tools in healthcare. This is a promising step towards harnessing the power of sound to improve patient outcomes.&lt;/p&gt;

&lt;p&gt;This blog post is based on the provided research article and does not include any outside information.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 01 Jul 2024 09:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/ml,/tech,/stroke/2024/07/01/TuneIn.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/ml,/tech,/stroke/2024/07/01/TuneIn.html</guid>
        
        
        <category>ML,</category>
        
        <category>Tech,</category>
        
        <category>Stroke</category>
        
      </item>
    
      <item>
        <title>Routine integration of palliative care into stroke unit care</title>
        <description>&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This manuscript presents a retrospective analysis examining the integration of palliative care into the care of stroke patients admitted to a regional stroke center. The study is important because despite the high morbidity and mortality associated with stroke, there is often a delay in initiating palliative care for these patients until death appears imminent. Early integration of palliative care has been shown to improve quality of life and symptom management in other serious illnesses like cancer.&lt;/p&gt;

&lt;p&gt;Key findings and take-away messages:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Only 28.8% of stroke patients who died in the hospital received a palliative medicine consultation (PMC), with a median time to consultation of 6 days from admission. This highlights missed opportunities for early palliative care integration.&lt;/li&gt;
  &lt;li&gt;Factors associated with a higher likelihood of receiving PMC included older age, female gender, absence of stroke diagnosis on admission, ischemic stroke type, and comorbidities of cancer or dementia.
Admission from another acute care hospital and lower Glasgow Coma Scale scores (indicating coma) were associated with a lower likelihood of PMC.&lt;/li&gt;
  &lt;li&gt;In multivariate analysis, only coma was significantly associated with a higher incidence of death, while no factors remained significantly associated with receiving PMC.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In summary, the results demonstrate an underutilization and delay in palliative care consultation for patients with severe strokes, even among those with the highest risk of death. The authors conclude that prospective studies in various stroke care settings are needed to better understand barriers and optimize the integration of palliative care into the management of acute stroke patients&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/38725344/&quot;&gt;Read the full paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 12 May 2024 09:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/palliativecare,/stroke/2024/05/12/StrokePalliativeCare.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/palliativecare,/stroke/2024/05/12/StrokePalliativeCare.html</guid>
        
        
        <category>Palliativecare,</category>
        
        <category>Stroke</category>
        
      </item>
    
      <item>
        <title>Deep Learning and Voice Analysis: New Frontiers in Stroke Diagnosis Insights from the Stroke Innovation Lab</title>
        <description>&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deep Learning and Voice Analysis: New Frontiers in Stroke Diagnosis Insights from the Stroke Innovation Lab&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Our first paper, work led by: Rami Saab and Arjun Balachandar&lt;/p&gt;

&lt;p&gt;Stroke is a leading cause of mortality and can result in long-term functional changes. A frequent and serious complication of stroke is dysphagia, or swallowing dysfunction, which occurs in around 55% of stroke patients. Dysphagia increases the risk of aspiration pneumonia, which can be fatal. Thus, screening for swallowing issues is a critical part of stroke patient care. Current screening methods have limitations, such as subjectivity and resource requirements, so there is a need for more efficient and objective tools.&lt;/p&gt;

&lt;p&gt;A new study published in &lt;em&gt;Frontiers in Neuroscience&lt;/em&gt; explores using &lt;strong&gt;machine learning to screen for post-stroke dysphagia&lt;/strong&gt; using vocal samples. This innovative approach uses deep learning to analyze voice changes, a key indicator of dysphagia.&lt;/p&gt;

&lt;p&gt;Here’s a breakdown of how the study was conducted and its key findings:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data Collection&lt;/strong&gt;: Researchers recorded the speech of 68 patients who had experienced a stroke, including sustained vowel sounds and speech samples from the National Institutes of Health Stroke Scale (NIHSS). The NIHSS is a validated tool for assessing neurological deficits in stroke patients.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Audio Processing&lt;/strong&gt;: The audio was segmented into 0.5-second clips and transformed into Mel-spectrogram images, which represent sound frequencies over time, and are designed to mimic the human ear’s perception of sound.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deep Learning Models&lt;/strong&gt;: The team used convolutional neural networks (CNNs) including DenseNet and ConvNext, which are effective for image-based classification tasks. They used transfer learning, which means the models were pre-trained on large image datasets and then fine-tuned to classify the audio-based spectrogram images. An ensemble approach was used to combine the results of both models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Outcomes&lt;/strong&gt;: The models were trained to classify patients as either “pass” or “fail” based on the Toronto Bedside Swallowing Screening Test (TOR-BSST©).
    &lt;ul&gt;
      &lt;li&gt;At the audio clip level, the ensemble model achieved a &lt;strong&gt;sensitivity of 71% and specificity of 77%&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;At the participant level, the ensemble model achieved a &lt;strong&gt;sensitivity of 89% and a specificity of 79%&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Key Findings:&lt;/strong&gt; The study showed that deep learning can classify vocalizations to detect post-stroke dysphagia. The use of both vowel sounds and the speech components of the NIHSS improved classification performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Why This Matters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This study is the first to show the feasibility of using deep learning to classify vocalizations for post-stroke dysphagia detection. This technology could improve dysphagia screening in several ways:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Reduced Subjectivity:&lt;/strong&gt; Machine learning can provide a more objective assessment compared to traditional methods.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improved Access:&lt;/strong&gt; This technology could allow for screening in settings with limited access to specialists, such as speech language pathologists (SLPs).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Remote Screening&lt;/strong&gt;: The use of voice analysis opens up the possibility of telehealth applications, which are especially relevant for remote patient care.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Limitations and Future Directions&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The authors note some limitations to the study including the small dataset size, which could limit the generalizability of the models. However, the data was collected in a real-world clinical setting, which enhances its applicability to other centers. The code developed for the project is also open source, which will facilitate wider use. Future work will involve using larger and more diverse datasets and also exploring automated methods for segmenting the audio data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This research demonstrates that machine learning, using deep learning models, offers a promising avenue for the development of non-invasive, objective, and rapid tools for post-stroke dysphagia screening. This technology has the potential to improve patient care and increase the availability of dysphagia screening.&lt;/p&gt;

&lt;p&gt;This blog post summarizes the key points of the research using information from the sources you provided, highlighting the importance and potential impact of this work in an accessible manner.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2023.1302132/full&quot;&gt;Machine-learning assisted swallowing assessment: a deep learning-based quality improvement tool to screen for post-stroke dysphagia&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CHIL 2024 ACCEPTED PAPER&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Post By: Eptehal Nashnoush, Hamza Mahdi, Rishit Dagli, Houman Khosravani&lt;/p&gt;

&lt;p&gt;Our lab (SiLab, Stroke Innovation Lab) has recently conducted an important comparative study demonstrating the potential of deep learning models to utilize voice as a biomarker for diagnosing stroke-related conditions. This research navigates the complex challenge of analyzing small datasets in clinical settings, offering promising solutions for neurology and other medical fields.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The Study at a Glance&lt;/em&gt;
Our research centered on analyzing audio data from stroke patients, employing advanced deep learning techniques to uncover new diagnostic possibilities. By focusing on two innovative datasets—one based on the National Institutes of Health Stroke Scale (NIHSS) speech segments and another on sustained vowel sounds—we sought to improve the understanding and diagnosis of swallowing disorders and assess stroke severity more accurately.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Leveraging Pre-training and Fine-tuning&lt;/em&gt;
A crucial aspect of our study was the strategic use of pre-training on extensive, publicly available datasets before fine-tuning on our specific clinical data. This approach significantly improved the accuracy of our models, showcasing the value of deep learning in clinical diagnostics. Below is a figure demonstrating how each technique works. We evaluated various neural network architectures, including Convolutional Neural Networks (CNNs) like DenseNet and ConvNeXt, and transformer models, to find the most effective method for audio classification with limited data.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Innovations in Spectrogram Analysis&lt;/em&gt;
We explored several spectrogram preprocessing techniques, including Mel RGB, Mel mono, and Superlet, to convert audio signals into visual formats for model analysis. Our findings indicate that while RGB preprocessing benefits from ImageNet pre-training, the Mel mono approach—especially when pre-trained on large audio datasets—outperforms RGB. This highlights the nuanced role of preprocessing in enhancing model performance.&lt;/p&gt;

&lt;p&gt;Mel RGB: This approach uses color in spectrogram representations and was particularly effective for the ConvNeXt model, which achieved an AUC of 0.91, ST of 0.78, and SP of 0.89, indicating a strong balance between accurately identifying both conditions and healthy controls.&lt;/p&gt;

&lt;p&gt;Mel mono: A grayscale single-channel representation, where the DenseNet model demonstrated a strong performance, notably with the DenseNet Contrastive pre-trained on US8K dataset, showing an AUC of 0.89, ST of 0.78, and a perfect SP of 1.00. Superlet: A newer method for spectrogram transformation, which generally resulted in lower performance across the models compared to the other preprocessing techniques.&lt;/p&gt;

&lt;p&gt;The ConvNeXt and DenseNet models outperformed other models like ViT and SWIN Transformer in certain conditions, indicating that the choice of model and preprocessing technique can significantly influence diagnostic accuracy in clinical audio analysis. This information is critical for developing effective tools for medical professionals and improving patient outcomes.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Clinical Applications and Future Directions&lt;/em&gt;
The research from the Stroke Innovation Lab not only demonstrates the effectiveness of CNN architectures in clinical audio classification but also opens up new avenues for using voice as a diagnostic tool across a range of medical conditions. Our findings represent a step towards more non-invasive, efficient, and accessible diagnostic methods, offering hope for both patients and healthcare professionals.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.10100&quot;&gt;Read the full arXiv paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Manuscript accepted at &lt;a href=&quot;https://chilconference.org/&quot;&gt;CHIL 2024&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Next Steps&lt;/em&gt;
The Stroke Innovation Lab is committed to further exploring the potential of audio classification in clinical settings. Our goal is to deepen our understanding of diseases and improve diagnostic accuracy through ongoing innovation in deep learning. We believe that voice analysis can play a crucial role in the future of healthcare diagnostics.&lt;/p&gt;

&lt;p&gt;Stay connected with the Stroke Innovation Lab for more updates on our journey to harness the power of science and technology in transforming medical diagnostics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 19 Feb 2024 08:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/ml,/tech,/stroke/2024/02/19/arXivMASA.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/ml,/tech,/stroke/2024/02/19/arXivMASA.html</guid>
        
        
        <category>ML,</category>
        
        <category>Tech,</category>
        
        <category>Stroke</category>
        
      </item>
    
      <item>
        <title>Watch the beta-blockers</title>
        <description>&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot;&gt;
  &lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;...&lt;/p&gt;&amp;mdash; ...
  &lt;a href=&quot;https://twitter.com/neuroccm/status/1629906707005440014&quot;&gt;...&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;I posted about this previously - did not get much traction :)
I am sure someone will go ahead and refute or prove this in a trial or looking back at the data.&lt;/p&gt;

&lt;p&gt;My clinical observation has been that the presence of beta-blockers, used for another indication (e.g. HTN), lowers the probability of detecting A Fib in a patient that presents with ESUS. Therefore, given the results of the recent ARCADIA trial &lt;a href=&quot;https://www.medscape.com/viewarticle/992525&quot;&gt;read more here&lt;/a&gt;, there is a suggestion that some markers of atrialcardiopathy may not be sufficient to detect the propensity of identifying those that may benefit from anticoagulation - note I don’t mean all biomarkers., some more useful than others, and some may cloud detection ability in a clinical trial. However, I suspect, the presence of BBs also modulates the detection of A Fib, and those with LA dilatation (best via the LAVI, volume index), and embolic stroke, who are on BB, I suspect these patients actually have a high probability of A fib, and thus the search should be on to find it. BB can reduce the probability of detection - I cannot find any papers that directly show this - but I have found this to be the case clinically over and over again. I also cannot reliably find reporting of BB use in A Fib large detection trials. There is some biologic plausability. Here is some “smoke” looking at the HR distributions with BB on-board, &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9996284/&quot;&gt;see this&lt;/a&gt;. The awesome point about this paper and other papers that are cited here are the fact that they show the complexity that we have with A Fib and how it is more than just atriopathy, but certainly involves HFpEF, chronic HTN, size and shape of the LAA, time in/out of A Fib, and even how medications used for HTN can modify intra-cardiac pressues to promote the development of A Fib - the cool-factor is that it is very intriguing and shows how no algebreic simple score can capture true risk of A Fib and risk of stroke from A Fib, as always, our models are approximations to what actually occures in nature. I am sure we will see more on this topic in the future! Given how complex A Fib is, it is not surprising its complexity cannot be captured via a simple algebreic sum of terms.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 09 Jul 2023 20:23:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/computing/2023/07/09/Watch_the_beta-blockers.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/computing/2023/07/09/Watch_the_beta-blockers.html</guid>
        
        
        <category>Computing</category>
        
      </item>
    
      <item>
        <title>Computing</title>
        <description>&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Computing!
Oh my how things have changed over the past few months.
ChatGPT, GPT4 model, and the continuous improvements with VSCode, and implementation of GPT4 as co-pilot (limited to code).
Overall these have been very positive developments.
Also, as a lab colleague showed me - Docker! Game changer - what an excellent concept to virtualize key components of dependencies in containers and be able to run them fast and efficiently, at fairly low-level, without the usual drag of virtualization.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 09 Jul 2023 20:23:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/computing/2023/07/09/Computing.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/computing/2023/07/09/Computing.html</guid>
        
        
        <category>Computing</category>
        
      </item>
    
      <item>
        <title>Musings on ChatGPT and its future use case(s): medicine and beyond</title>
        <description>&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;With the dawn of AI technologies and the fast pace of development in this field, there is often fear, hesitation, and concern that human jobs will be lost or replaced. That said, there are fundamental ethical and legal considerations with AI systems in general and healthcare in particular. There are also implications for medical education and our trainees.&lt;/p&gt;

&lt;p&gt;With the recent advancement of large language models and OpenAI’s public beta of ChatGPT, we are seeing a real glimpse into some of the capabilities. In this specific context, it is more likely that the jobs impacted are not the so-called blue-collar jobs or service jobs but rather white-collar jobs that deal with news services, advertisement, communication, copyright, and writing in various domains. Anything that deals with caring for people (such as in medicine) is likely safe for some time in the near future - but I digress. Of course, this is merely a supposition, and yet this is shared and commented on by several technology journalists covering recent advances in these large language models. The other aspect that requires some consideration is how these tools affect education, specifically in early education, entrance essays, and academic settings. There are already tools that detect plagiarism, and the fact that these language models can compose lengthy written works with simple typed instruction brings into light some concerns about ensuring a good quality of education for our undergraduate and early-year trainees. Will we see the return of in-person tests, with pen and paper, to ensure that a human rather than a language model (some call these stochastic parrots) is not writing an essay, composition, or report? Perhaps - we will see. These are tremendous advances, and their appropriate use is likely in the setting of augmenting and aiding humans in their writing and composition. Possibly, the reasonable use case for these language models is to develop an outline or a series of paragraphs to expand on, with original human content and writing prowess, to create an excellent composition.&lt;/p&gt;

&lt;p&gt;For now, let us take this moment to celebrate this advancement in AI/machine learning as the series of paragraphs written in response to my question/input to ChatGPT, “Write several paragraphs on why rapid presentation to hospital is important for acute stroke care” is, in fact, quite remarkable!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 19 Dec 2022 00:10:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/stroke/2022/12/19/Musings_on_ChatGPT.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/stroke/2022/12/19/Musings_on_ChatGPT.html</guid>
        
        
        <category>Stroke</category>
        
      </item>
    
      <item>
        <title>ChatGPT is asked - Write several paragraphs on why rapid presentation to hospital is important for acute stroke care</title>
        <description>&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The following is from OpenAI’s ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Rapid presentation to the hospital is important for acute stroke care because time is a critical factor in the treatment of stroke. The sooner a person receives treatment for a stroke, the better their chances of making a full recovery and avoiding long-term complications.&lt;/p&gt;

&lt;p&gt;When a person experiences a stroke, the blood supply to their brain is disrupted, which can cause brain cells to die. The longer this disruption continues, the more brain tissue is damaged, and the more severe the effects of the stroke will be. Therefore, it is essential to restore blood flow to the brain as quickly as possible to minimize the damage.&lt;/p&gt;

&lt;p&gt;Rapid presentation to the hospital allows doctors to quickly diagnose the type of stroke and determine the best course of treatment. There are several different treatments available for stroke, and the choice of treatment will depend on the type of stroke and the extent of the damage. For example, some treatments aim to restore blood flow to the brain by removing a blood clot that is blocking an artery, while others may involve medications to prevent further damage to brain tissue.&lt;/p&gt;

&lt;p&gt;It is also important to note that some treatments for stroke, such as thrombolysis, must be administered within a specific time window in order to be effective. This time window is typically within the first few hours after the onset of symptoms. Therefore, rapid presentation to the hospital is essential to ensure that the person experiencing a stroke receives the appropriate treatment as quickly as possible.&lt;/p&gt;

&lt;p&gt;In addition, rapid presentation to the hospital can also help to prevent further complications from occurring. For example, a person who has experienced a stroke may be at risk of developing pneumonia, which can be life-threatening. By seeking treatment quickly, doctors can provide the person with the appropriate care and support to prevent complications from occurring.&lt;/p&gt;

&lt;p&gt;Overall, rapid presentation to the hospital is important for acute stroke care because it allows doctors to quickly diagnose the type of stroke and provide the appropriate treatment to minimize the damage and prevent further complications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Well written, the future is bright for AI aiding in public messaging ad campaigns that require documents, advertising, and messaging for healthcare.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 11 Dec 2022 00:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/stroke/2022/12/11/ChatGPT_Stroke.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/stroke/2022/12/11/ChatGPT_Stroke.html</guid>
        
        
        <category>Stroke</category>
        
      </item>
    
      <item>
        <title>The Devil is in the Details - Understanding CT Perfusion in the late EVT windows</title>
        <description>&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;CT perfusion (CTP) is a powerful imaging technique that can be utilized in order to distinguish the region of infarction from the ischemic penumbra.  CT perfusion algorithms are typically proprietary, and are always dependent on hemodynamic factors.  Such factors include presence of extracranial stenosis, presence of sinus rhythm (perfusion may be affected by atrial fibrillation), systemic factors such as hypoperfusion/shock, and more technical factors such as accuracy by which contrast is given, location of selection for both the arterial and venous phase functions, and patient motion during the scan.  In an ideal study, CT perfusion is able to demonstrate the 3 typical maps that are used to make decisions, including his cerebral blood flow map [CBF], mean transit time [MTT], and the cerebral blood volume [CBV].&lt;/p&gt;

&lt;p&gt;With thrombectomy now occurring in extended time windows (DEFUSE3 and DAWN trials), the role of CT perfusion has been markedly expanded as CT perfusion, when used appropriately, can provide insight into what brain regions can be salvageable from an ischemia and penumbra perspective.&lt;/p&gt;

&lt;p&gt;As usual the devil is in the details.&lt;/p&gt;

&lt;p&gt;With regards to DEFUSE3 and DAWN, the following table from a topical review by Dr. Alberts, outlines the criteria used to distinguish ischemic core and mismatch volume and the presence of a large vessel occlusion.  Similar criteria can be used an MRI imaging (perfusion imaging), however this article will focus on CTP.&lt;/p&gt;

&lt;p&gt;Of note, median ischemic core volumes in both trials, are approximately 10 cc, and as evident in the table below, DEFUSE3 included ischemic core volumes up to 70 cc, with a graded approach, dependent on age and NIH stroke scale, as utilized in the DAWN trial.&lt;/p&gt;

&lt;p&gt;The ischemic core volume is felt to be estimated by cerebral blood flow less than 30% of normal.  This value is felt to correlate to perfusion at the cellular level being less than 20cc/100g/min of brain were critical ischemia occurs.  Recall that normal cerebral blood flow is on the order of 50-60cc/100g/min.&lt;/p&gt;

&lt;p&gt;Campbell et al., showed that 30% of normal CBF was the optimal threshold to define the ischemic core within a Tmax &amp;gt; 6s (penumbra region), when CT perfusion was processed by standard singular value decomposition.  It should be noted that this decomposition mathematically assumes no delay in blood flow from proximal arteries to the ischemic brain tissue whereas T-max and CBF are measured. The threshold of using CBF &amp;lt; 30%, is further supported by data from the SWIFT PRIME trials, where the RAPID software was used, with this threshold of CBF strongly correlating with the final infarct volume at 27 hours inpatient to achieved reperfusion. In the study by Cereda et al., as CBF threshold of less than 38% of normal predicted DWI volumes.  However in some patients this threshold significantly overestimated the size of the DWI lesion, and therefore threshold of 30% has been used in the late window EVT trials. Cereda et al., show that a threshold of approximately 38% showed the least mean volume difference (DWI-CTP in cc), of the ischemic core.  It should be noted that if a lower percentage is selected such as 25% of normal CBF, that this underestimates the core vs. DWI (i.w. DWI-CTP becomes a larger positive number with lowering of CBF threshold under 38% -&amp;gt; towards 2%).&lt;/p&gt;

&lt;p&gt;Similarly T-max greater than 6 seconds has been used to define the ischemic penumbra.&lt;/p&gt;

&lt;p&gt;Aside from the fact that these computations are hemodynamic in nature, T-max also provides information with regards to collaterals.  And regions of brain with T-max greater than 10 seconds are felt to correlate with regions of poor collateral flow.&lt;/p&gt;

&lt;p&gt;The ratio of tissue volume with T-max greater than 10 seconds compared to T-max greater than 6 seconds is referred to as the hypoperfusion intensity ratio, and it is notable that the high rations, greater than 0.5 (i.e. 50%) correlate with poor angiographic collaterals and these patients likely have a March larger ischemic core then visualized, and the score is more likely to rapidly grow.&lt;/p&gt;

&lt;p&gt;Therefore in order to use CTP effectively, not only the fact that it is a hemodynamic calculation needs to be considered, the patient has to meet various factors as outlined, but also the details of these thresholds whereby the ischemic core and collateral status are approximated, need to be considered in order to make proper decisions with regards to the information provided by the RAPID software.&lt;/p&gt;

&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Albers, Use of Imaging to Select Patients for Late Window Endovascular Therapy,Stroke. 2018;49:00-00. DOI: 10.1161/STROKEAHA.118.021011.&lt;/li&gt;
  &lt;li&gt;Cereda CW, Christensen S, Campbell BC, Mishra NK, Mlynash M, Levi C, et al. A benchmarking tool to evaluate computer tomography perfu- sion infarct core predictions against a DWI standard. J Cereb Blood Flow Metab. 2016;36:1780–1789. doi: 10.1177/0271678X15610586&lt;/li&gt;
  &lt;li&gt;Campbell BC, Christensen S, Levi CR, Desmond PM, Donnan GA, Davis SM, et al. Cerebral blood flow is the optimal CT perfusion param- eter for assessing infarct core. Stroke. 2011;42:3435–3440. doi: 10.1161/ STROKEAHA.111.618355.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 12 Sep 2018 13:11:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/stroke/2018/09/12/Details_of_CTP.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/stroke/2018/09/12/Details_of_CTP.html</guid>
        
        
        <category>Stroke</category>
        
      </item>
    
      <item>
        <title>WAKE-UP - who can get TPA outside of the 4.5 hr time-window?</title>
        <description>&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A proportion of patients wake up with a stroke and it is believed that in a significant fraction of these patients, the stroke occurs close to the waking hour.  Such patients, are traditionally excluded from thrombolytic treatment given that they are outside of the standard window for delivery of TPA, this being 4.5 hours (i.e. last seen normal beyond 4.5 hrs).&lt;/p&gt;

&lt;p&gt;In a study published in May Thomalla et al. (2018) in the New England Journal of Medicine, the authors of the WAKE-UP trial tested a hypothesis that some of these patients can be candidates for TPA, when guided by MRI imaging, specifically mismatch between DWI and FLAIR.&lt;/p&gt;

&lt;p&gt;Patient that met inclusion criteria had to be, at minimum, outside of the 4.5hrs from last seen normal. The median interval was approximately 7 hrs in both the TPA and placebo groups (from last known to be well). This was a prospective, investigator-initiated multi-center, randomized, double-blind, placebo-controlled clinical trial. Patients could undergo randomization if in the judgment of the investigator MRI showed an acute ischemic lesion on DWI imaging but not parenchymal hyperintensity (using standard) FLAIR windows. Below I have linked to the paper (imaging-based) where the scientific rational for looking beyond 4.5hrs comes from in way of DWI FLAIR mismatch (Thomalla et al. 2011).&lt;/p&gt;

&lt;p&gt;In this study, 503 suitable imaging results, were randomly assigned to alteplase or placebo, and most of the patient’s in the study had mild to moderate strokes.  Primary endpoint was favorable outcome mRS at 90 days of 0 or 1. This occurred 53.3% (TPA) vs. 41.8% (Placebo group) - (adjusted odds ratio, 1.61; 95% confidence interval [CI], 1.09 ­ 2.36; p = 0.02).&lt;/p&gt;

&lt;p&gt;It is important to note that this trial stopped early due to funding (approx. 2/3 of target enrollment).&lt;/p&gt;

&lt;p&gt;There was a trend towards increased mortality in the TPA group, but that it did not achieve statistical significance (4.1% vs. 1.2%, p=0.07). Of the 10 patients that died, 4 were due to symptomatic ICH. Rate of parenchymal hemorrhage type 2 was 4% vs 0.4% (p=0.03).&lt;/p&gt;

&lt;p&gt;Challenges in implementation include the fact that obtaining MRI imaging in a timely manner is difficult and most centers. Also a large proportion of the patients in this trial were also eligible for EVT as per DAWN and DIFFUSE3 criteria  - approximately 20% of patients, which raises the possibility (unknown) of better outcome with EVT vs. late-window TPA administration.&lt;/p&gt;

&lt;p&gt;Taken together, the findings of WAKE-UP push the boundaries of TPA administration in acute stroke care beyond the 4.5hr time window. This especially benefits those who may have had their stroke close to waking without known a “last seen normal” time - these patients may be then identified via imaging. For now, my personal opinion (comfort) is to stay within the ascribed 4.5hrs (reserving out of window TPA for special circumstances, off-label with open disclosure to the patient/SDM). It’s great however that such studies exist providing additional scientific basis for out of window TPA. Collectively some criticisms of the trial include: early termination/lack of full enrollment, large fraction of patients were EVT candidates, signal towards higher mortality, and high risk of bleeding.&lt;/p&gt;

&lt;p&gt;The researchers discovered that in patients who received TPA, they had an increased chance of a favorable outcome (mRS 0-1), at 90 days.  Indeed, this was the first positive trial for IV thrombolysis and strokes of unknown time, however there are many important aspects to the study to consider.&lt;/p&gt;

&lt;p&gt;Nonetheless, studies such as WAKE-UP emphasize the notion that when identifying brain tissue at risk/amenable to being saved, it’s less about time and more about the state of the underlying tissue.  In other words, we are on a journey to better understand the ischemic penumbra about areas of brain at risk versus areas that have completed infarction -  Tissue window (perfusion, cellular energetics, collaterals) rather than time windows based on the clinical history.&lt;/p&gt;

&lt;p&gt;Until next time…more on collaterals!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.nejm.org/doi/full/10.1056/NEJMoa1804355&quot;&gt;WAKE-UP NEJM Trial, Thomalla et al.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.medscape.com/viewarticle/896775&quot;&gt;WAKE-UP Medscape Editorial by Sue Hughes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/21978972&quot;&gt;Science behind the 4.5hr time-window for DWI/FLAIR mismatch, Thomalla et al.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Thomalla, G., Simonsen, C. Z., Boutitie, F., Andersen, G., Berthezene, Y., Cheng, B., et al. (2018). MRI-Guided Thrombolysis for Stroke with Unknown Time of Onset. The New England Journal of Medicine, 379(7), 611–622.&lt;/li&gt;
  &lt;li&gt;(editorial NEJM on WAKE-UP) Jovin, T. G. (2018). MRI-Guided Intravenous Alteplase for Stroke — Still Stuck in Time. The New England Journal of Medicine, 379(7), 682–683.&lt;/li&gt;
  &lt;li&gt;Thomalla, G., Cheng, B., Ebinger, M., Lancet, Q. H. T., 2011. (n.d.). DWI-FLAIR mismatch for the identification of patients with acute ischaemic stroke within 4· 5 h of symptom onset (PRE-FLAIR): a multicentre observational study. Elsevier.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;/posts/index.html&quot;&gt;Back to posts…&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 01 Sep 2018 13:11:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/stroke/2018/09/01/WAKE_UP.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/stroke/2018/09/01/WAKE_UP.html</guid>
        
        
        <category>Stroke</category>
        
      </item>
    
  </channel>
</rss>
