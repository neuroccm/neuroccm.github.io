I"ç%<p><a href="/posts/index.html">Back to posts‚Ä¶</a></p>

<p><strong>Deep Learning and Voice Analysis: New Frontiers in Stroke Diagnosis Insights from the Stroke Innovation Lab</strong></p>

<p>Our first paper, work led by: Rami Saab and Arjun Balachandar</p>

<p>Okay, here‚Äôs a blog post about the research on using machine learning to detect post-stroke dysphagia, based on the provided sources:</p>

<p><strong>Machine Learning Offers New Hope for Post-Stroke Dysphagia Screening</strong></p>

<p>Stroke is a leading cause of mortality and can result in long-term functional changes. A frequent and serious complication of stroke is dysphagia, or swallowing dysfunction, which occurs in around 55% of stroke patients. Dysphagia increases the risk of aspiration pneumonia, which can be fatal. Thus, screening for swallowing issues is a critical part of stroke patient care. Current screening methods have limitations, such as subjectivity and resource requirements, so there is a need for more efficient and objective tools.</p>

<p>A new study published in <em>Frontiers in Neuroscience</em> explores using <strong>machine learning to screen for post-stroke dysphagia</strong> using vocal samples. This innovative approach uses deep learning to analyze voice changes, a key indicator of dysphagia.</p>

<p>Here‚Äôs a breakdown of how the study was conducted and its key findings:</p>

<ul>
  <li><strong>Data Collection</strong>: Researchers recorded the speech of 68 patients who had experienced a stroke, including sustained vowel sounds and speech samples from the National Institutes of Health Stroke Scale (NIHSS). The NIHSS is a validated tool for assessing neurological deficits in stroke patients.</li>
  <li><strong>Audio Processing</strong>: The audio was segmented into 0.5-second clips and transformed into Mel-spectrogram images, which represent sound frequencies over time, and are designed to mimic the human ear‚Äôs perception of sound.</li>
  <li><strong>Deep Learning Models</strong>: The team used convolutional neural networks (CNNs) including DenseNet and ConvNext, which are effective for image-based classification tasks. They used transfer learning, which means the models were pre-trained on large image datasets and then fine-tuned to classify the audio-based spectrogram images. An ensemble approach was used to combine the results of both models.</li>
  <li><strong>Outcomes</strong>: The models were trained to classify patients as either ‚Äúpass‚Äù or ‚Äúfail‚Äù based on the Toronto Bedside Swallowing Screening Test (TOR-BSST¬©).
    <ul>
      <li>At the audio clip level, the ensemble model achieved a <strong>sensitivity of 71% and specificity of 77%</strong>.</li>
      <li>At the participant level, the ensemble model achieved a <strong>sensitivity of 89% and a specificity of 79%</strong>.</li>
    </ul>
  </li>
  <li><strong>Key Findings:</strong> The study showed that deep learning can classify vocalizations to detect post-stroke dysphagia. The use of both vowel sounds and the speech components of the NIHSS improved classification performance.</li>
</ul>

<p><strong>Why This Matters</strong></p>

<p>This study is the first to show the feasibility of using deep learning to classify vocalizations for post-stroke dysphagia detection. This technology could improve dysphagia screening in several ways:</p>

<ul>
  <li><strong>Reduced Subjectivity:</strong> Machine learning can provide a more objective assessment compared to traditional methods.</li>
  <li><strong>Improved Access:</strong> This technology could allow for screening in settings with limited access to specialists, such as speech language pathologists (SLPs).</li>
  <li><strong>Remote Screening</strong>: The use of voice analysis opens up the possibility of telehealth applications, which are especially relevant for remote patient care.</li>
</ul>

<p><strong>Limitations and Future Directions</strong></p>

<p>The authors note some limitations to the study including the small dataset size, which could limit the generalizability of the models. However, the data was collected in a real-world clinical setting, which enhances its applicability to other centers. The code developed for the project is also open source, which will facilitate wider use. Future work will involve using larger and more diverse datasets and also exploring automated methods for segmenting the audio data.</p>

<p><strong>Conclusion</strong></p>

<p>This research demonstrates that machine learning, using deep learning models, offers a promising avenue for the development of non-invasive, objective, and rapid tools for post-stroke dysphagia screening. This technology has the potential to improve patient care and increase the availability of dysphagia screening.</p>

<p>This blog post summarizes the key points of the research using information from the sources you provided, highlighting the importance and potential impact of this work in an accessible manner.</p>

<p><a href="https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2023.1302132/full">Machine-learning assisted swallowing assessment: a deep learning-based quality improvement tool to screen for post-stroke dysphagia</a></p>

<p>Post By: Eptehal Nashnoush, Hamza Mahdi, Rishit Dagli, Houman Khosravani</p>

<p>Our lab (SiLab, Stroke Innovation Lab) has recently conducted an important comparative study demonstrating the potential of deep learning models to utilize voice as a biomarker for diagnosing stroke-related conditions. This research navigates the complex challenge of analyzing small datasets in clinical settings, offering promising solutions for neurology and other medical fields.</p>

<p><em>The Study at a Glance</em>
Our research centered on analyzing audio data from stroke patients, employing advanced deep learning techniques to uncover new diagnostic possibilities. By focusing on two innovative datasets‚Äîone based on the National Institutes of Health Stroke Scale (NIHSS) speech segments and another on sustained vowel sounds‚Äîwe sought to improve the understanding and diagnosis of swallowing disorders and assess stroke severity more accurately.</p>

<p><em>Leveraging Pre-training and Fine-tuning</em>
A crucial aspect of our study was the strategic use of pre-training on extensive, publicly available datasets before fine-tuning on our specific clinical data. This approach significantly improved the accuracy of our models, showcasing the value of deep learning in clinical diagnostics. Below is a figure demonstrating how each technique works. We evaluated various neural network architectures, including Convolutional Neural Networks (CNNs) like DenseNet and ConvNeXt, and transformer models, to find the most effective method for audio classification with limited data.</p>

<p><em>Innovations in Spectrogram Analysis</em>
We explored several spectrogram preprocessing techniques, including Mel RGB, Mel mono, and Superlet, to convert audio signals into visual formats for model analysis. Our findings indicate that while RGB preprocessing benefits from ImageNet pre-training, the Mel mono approach‚Äîespecially when pre-trained on large audio datasets‚Äîoutperforms RGB. This highlights the nuanced role of preprocessing in enhancing model performance.</p>

<p>Mel RGB: This approach uses color in spectrogram representations and was particularly effective for the ConvNeXt model, which achieved an AUC of 0.91, ST of 0.78, and SP of 0.89, indicating a strong balance between accurately identifying both conditions and healthy controls.</p>

<p>Mel mono: A grayscale single-channel representation, where the DenseNet model demonstrated a strong performance, notably with the DenseNet Contrastive pre-trained on US8K dataset, showing an AUC of 0.89, ST of 0.78, and a perfect SP of 1.00. Superlet: A newer method for spectrogram transformation, which generally resulted in lower performance across the models compared to the other preprocessing techniques.</p>

<p>The ConvNeXt and DenseNet models outperformed other models like ViT and SWIN Transformer in certain conditions, indicating that the choice of model and preprocessing technique can significantly influence diagnostic accuracy in clinical audio analysis. This information is critical for developing effective tools for medical professionals and improving patient outcomes.</p>

<p><em>Clinical Applications and Future Directions</em>
The research from the Stroke Innovation Lab not only demonstrates the effectiveness of CNN architectures in clinical audio classification but also opens up new avenues for using voice as a diagnostic tool across a range of medical conditions. Our findings represent a step towards more non-invasive, efficient, and accessible diagnostic methods, offering hope for both patients and healthcare professionals.</p>

<p><a href="https://arxiv.org/abs/2402.10100">Read the full arXiv paper</a>.</p>

<p><strong>Manuscript accepted at <a href="https://chilconference.org/">CHIL 2024</a></strong></p>

<p><em>Next Steps</em>
The Stroke Innovation Lab is committed to further exploring the potential of audio classification in clinical settings. Our goal is to deepen our understanding of diseases and improve diagnostic accuracy through ongoing innovation in deep learning. We believe that voice analysis can play a crucial role in the future of healthcare diagnostics.</p>

<p>Stay connected with the Stroke Innovation Lab for more updates on our journey to harness the power of science and technology in transforming medical diagnostics.</p>

<p><a href="/posts/index.html">Back to posts‚Ä¶</a></p>
:ET